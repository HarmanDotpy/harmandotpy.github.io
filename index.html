<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Harman Singh</title>
  <meta name="google-site-verification" content="tTnhER1B7VpkPPCjXelukKzHmzNz4ptE32kZ2CFTXBc" />
  <meta name="author" content="Harman Singh">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Harman Singh</name>
              </p>
              <p>I am a researcher at <a href="https://research.google/teams/india-research-lab/">Google DeepMind</a>, working on large scale multimodal and multilingual modelling with <a href="https://parthatalukdar.github.io/">Dr. Partha Talukdar</a> in the Languages team. 
              </p>
              <p>Previously, I was an AI Resident at <a href=" https://ai.facebook.com/">Meta AI</a> where I worked on reasoning abilities of Vision Language Models.
                Before this, I completed my undergrad from <a href="https://home.iitd.ac.in/">Indian Institute of Technology, Delhi</a>, advised by <a href="https://www.cse.iitd.ac.in/~parags/">Prof. Parag Singla</a>.
                During this time, I have been a research intern at <a href="https://inklab.usc.edu/">InkLab, USC</a>, advised by <a href="https://shanzhenren.github.io/">Prof. Xiang Ren</a> and 
                a research intern at <a href="https://research.ibm.com/artificial-intelligence">IBM Research AI</a>. <br>  
              </p> 
              <p style="text-align:center">
                <a href="mailto:harmansingh.iitd@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/harman_cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=BanlVLYAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.semanticscholar.org/author/Harman-Singh/2119151340">Semantic Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/Harman26Singh">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/HarmanDotpy/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/harman_cookie.jpg"><img style="width:80%;max-width:80%" alt="profile photo" src="images/harman_cookie.jpg" onclick="return false;"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Research</h2>
              <p>
                I am interested in multimodal modeling, reasoning, post-training and long-context capabilities of foundation models. Along with this, I am interested in building inclusive language technologies that can benefit a broader range of users.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr height="300px">
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img src="images/gemma3.png" alt="Gemma" width="100%" >
            </td>
            <td width="100%" valign="middle">
              <a href="https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf">
                <papertitle>Gemma 3</papertitle>
              </a>
              <br>
              <strong>Contributor</strong></a>
              <br>
              <a href="https://blog.google/technology/developers/gemma-3/">Google Blog</a> | <a href="https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf">Gemma 3 Technical Report</a>
            </td>
          </tr>

          <tr height="300px">
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img src="images/IndicGenBench.png" alt="IndicGenBench" width="100%" >
            </td>
            <td width="100%" valign="middle">
              <a href="https://arxiv.org/abs/2404.16816">
                <papertitle>IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages</papertitle>
              </a>
              <br>
              <strong>Harman Singh</strong>, Nitish Gupta, Shikhar Bharadwaj, Dinesh Tewari, Partha Talukdar</a>
              <br>
              <br>
              <strong>ACL 2024 (long paper, main conference)</strong>
              <br>
              Dataset Links: <a href="https://github.com/google-research-datasets/indic-gen-bench">Github</a> |  <a href="https://huggingface.co/collections/google/indicgenbench-663185166ea914038209b723">HuggingFace</a>
              <p>IndicGenBench is a multilingual, multi-way parallel benchmark for measuring language generation capabilities across diverse user-facing tasks in 29 Indic languages spanning 13 writing scripts and 4 language families.</p>
            </td>
          </tr>

          <tr height="300px">
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img src="images/MosaiCLIP_2.png" alt="MosaiCLIP" width="100%" >
            </td>
            <td width="100%" valign="middle">
              <a href="https://arxiv.org/abs/2305.13812">
                <papertitle>Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality</papertitle>
              </a>
              <br>
              <strong>Harman Singh</strong>, Pengchuan Zhang, Qifan Wang, Mengjiao Wang, Wenhan Xiong, Jingfei Du, Yu Chen</a>
              <br>
              <br>
              <strong>EMNLP 2023 (long paper, main conference)</strong>
              <br>
              <em style="color: red;">Oral acceptance to <a href="https://iccv-clvl.github.io/2023/">CLVL Workshop</a> at ICCV 2023</em>
              <br>
              <em style="color: red;">Oral acceptance to <a href="https://splu-robonlp-2023.github.io/">SpLU-RoboNLP Workshop</a> at EMNLP 2023</em>
              <br>
              <p>Improving compositional reasoning capabilities of SOTA Vision-Language Models through a new Coarse-to-Fine contrastive learning technique as well as effective hard negative mining.</p>
            </td>
          </tr>

          <tr height="300px">
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img src="images/multi-hop-ke.png" alt="Multi-Hop-KE" width="100%" >
            </td>
            <td width="100%" valign="middle">
              <a href="https://arxiv.org/abs/2407.10275">
                <papertitle>Cross-Lingual Multi-Hop Knowledge Editing</papertitle>
              </a>
              <br>
              Aditi Khandelwal*, <strong>Harman Singh*</strong>, Hengrui Gu, Tianlong Chen, Kaixiong Zhou</a> [*Equal Contribution]
              <br>
              <br>
              <strong>EMNLP 2024 (long paper, findings)</strong>
              <br>
              <p>Benchmarking and improving retrieval augmented knoweledge editing of LLMs, in a cross-lingual and multi-hop setting.</p>
            </td>
          </tr>

          <tr height="300px">
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img src="images/NeuroSIM.png" alt="NeuroSIM" width="100%" >
            </td>
            <td width="100%" valign="middle">
              <a href="https://arxiv.org/abs/2305.14410">
                <papertitle>Image Manipulation via Multi-Hop Instructions -- A New Dataset and Weakly-Supervised Neuro-Symbolic Approach</papertitle>
              </a>
              <br>
              <strong>Harman Singh</strong>, Poorva Garg, Mohit Gupta, Kevin Shah, Ashish Goswami, Satyam Modi, Arnab Kumar Mondal, Dinesh Khandelwal, Parag Singla, Dinesh Garg</a>
              <br>
              <br>
              <strong>EMNLP 2023 (long paper, main conference)</strong>
              <br>
              <p>New datasets and a modular method for weakly-supervised instruction guided image manipulations.</p>
            </td>
          </tr>

          <tr height="300px">
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img src="images/fairr.png" alt="fairr" width="100%" >
            </td>
            <td width="100%" valign="middle">
              <a href="https://aclanthology.org/2022.acl-long.77/">
                <papertitle>FaiRR: Faithful and Robust Deductive Reasoning over Natural Language</papertitle>
              </a>
              <br>
              Soumya Sanyal, <strong>Harman Singh</strong>, Xiang Ren</a>
              <br>
              <br>
              <strong>ACL 2022 (long paper, main conference)</strong>
              <br>
              <p>Proposed a modular method (FaiRR) for logical reasoning over natural language rule bases. Our methods ensure model faithfulness by assured causal relation from the proof step to the inference reasoning. FaiRR is more interpretable, efficient as compared to baselines, and generalizes better to OOD logical reasoning tasks.</p>
            </td>
          </tr>

          <tr height="300px">
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img src="images/STAB.png" alt="STAB" width="100%" >
            </td>
            <td width="100%" valign="middle">
              <a href="https://arxiv.org/abs/2409.02384">
                <papertitle>STAB: Speech Tokenizer Assessment Benchmark</papertitle>
              </a>
              <br>
              Shikhar Vashisht*, <strong>Harman Singh*</strong>, Shikhar Bharadwaj*, Sriram Ganapathy, Chulayuth Asawaroengchai, Kartik Audhkhasi, Andrew Rosenberg, Ankur Bapna, Bhuvana Ramabhadran</a> [*Equal Contribution]
              <br>
              <br>
              <strong>Under Review</strong>
              <br>
              <p>A systematic evaluation framework designed to assess speech tokenizers comprehensively and shed light on their inherent characteristics, cheaply, without having to train large speech foundation models.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!-- Title Row -->
          <tr>
            <td colspan="2" align="center">
              <h2>Past Work (Bioinformatics Research)</h2>
            </td>
          </tr>

          <tr height="300px">
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img src="images/covid-paper-nature-image.png" alt="covid-paper-nature-image" width="100%" >
            </td>
            <td width="100%" valign="middle">
              <a href="https://www.nature.com/articles/s41592-022-01444-z">
                <papertitle>Unlocking capacities of genomics for the COVID-19 response and future pandemics</papertitle>
              </a>
              <br>
              Sergey Knyazev, Karishma Chhugan, <strong>Harman Singh*</strong>, Varuni Sarwal*, Ram Ayyala*, ..., Serghei Mangul</a>
              <br>
              <em><strong>Nature Methods</strong></em>
              <br>
              <!-- <p>Improving compositional reasoning capabilities of SOTA Vision-Language Models through a new Coarse-to-Fine contrastive learning technique as well as effective hard negative mining.</p> -->
            </td>
          </tr>

          <tr height="300px">
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img src="images/e-MSN-isbra.png" alt="e-MSN-isbra" width="100%" >
            </td>
            <td width="100%" valign="middle">
              <a href="https://dl.acm.org/doi/abs/10.1007/978-3-030-91415-8_15">
                <papertitle>A Novel Network Representation of SARS-CoV-2 Sequencing Datas</papertitle>
              </a>
              <br>
              Sergey Knyazev, Daniel Novikov, Mark Grinshpon, <strong>Harman Singh</strong>, ..., Serghei Mangul</a>
              <br>
              <em><strong>International Symposium on Bioinformatics Research and Applications 2021</strong></em>
              <br>
              <!-- <p>Improving compositional reasoning capabilities of SOTA Vision-Language Models through a new Coarse-to-Fine contrastive learning technique as well as effective hard negative mining.</p> -->
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              Reviewer for ICLR 2023, NeurIPS 2023, EMNLP 2023, MLRC 2022 <span style="color: red;">(Outstanding Reviewer Award)</span>, COLM 2024, ACL Rolling Review (ARR) 2024 (Feb, April, June)
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              Teaching Assitant for Machine Learning (Dr. Sumeet Agarwal and Dr. Jayadeva, Fall 2021)
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              Teaching Assitant for Intro. to EE (Dr. Anuj Dhawan, Fall 2021)
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              Demo Leader, NeurIPS Education Outreach Program (for 240+ high school students), NeurIPS 2022
            </td>
          </tr>
        </tbody></table>
        <small><center>Website template by <a href="https://github.com/jonbarron/website">Jon Barron</a></center></small>
      </td>



    
    </tr>
  </table>
</body>

</html>
